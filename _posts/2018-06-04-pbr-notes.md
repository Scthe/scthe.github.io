---
title: "Physically based rendering notes"
excerpt: "Physically based rendering - equations + explanation"
date: 2018-06-04 12:00:00
---

{% capture image_dir %}{{ site.url }}/images/2018-06-04-pbr-notes{% endcapture %}

{::comment}
TODO check all notation of upper/little case letters
TODO sources - bibtex + links in text
TODO close button design
TODO all comments
TODO check mobile - all equations
{:/comment}


This is an electronic version of my notes created during [webgl-pbr](//github.com/Scthe/webgl-pbr) project. There may be some mistakes, so be vary. This article focuses on point lights and opaque (non-transparent) surfaces for brevity.

## Dictionary

Let's start with list of all terms used through this article. Most of them are going to be explained in more details later.

Terms:

* **albedo/diffuse** - base color of an object, usually represented as a texture.
* **roughness** - quite intuitive this one. Mirror or water are smooth (low roughness), while rock is rough (high roughness). Rough materials reflect light in more directions than smooth ones. This material parameter controls how blurry the reflections are. Usually defines roughness at microscopic scale.
* **glossiness** - opposite of roughness ($$glossiness = 1 - roughness$$). It's sometimes easier for artists to operate in this scale (white means smooth surface, black means rough).
* **metallic** - the metallic-ness (0 = dielectric, 1 = metallic) of a material. The metallic object has no diffuse component and a tinted incident specular, equal to the albedo color.
* **ambient occlusion (ao)** - trick used to approximate global illumination. Describes how *accesible* the surface is to light i.e. less accessible surface is generally darker.
* **Bidirectional Reflectance Distribution Function (BRDF)** - function  $$ f_r (\theta_l, \phi_l, \theta_v, \phi_v) $$ that describes how light is reflected from an opaque surface.
* **attenuation** - light fall-off over distance.
* **incidence angle** - angle of 0dgr. Very often in equations we will get base value of certain property at incidence angle, and then write function to approximate same value for different angles.
* **radiance** - measure of amount of light coming from angle $$ \theta $$.
* **irradiance** - sum of all incoming light (radiance).

Notation:

* **n** - normal vector of a surface (vector perpendicular to the surface)
* **l** - normalized vector from point of the surface towards the light source
* **v** - normalized vector from point of the surface towards the camera
* **m** - normal of a single microfacet
* $$F_0$$ - specular reflectance at incidence angle
* **h** -  halfway vector, calculated from following equation: $$h = \frac {l+v}{\|l+v\|}$$
* $$dot(n,l)$$ - dot product of vectors n,l. Usually I assume that both n, l are normalized
* **vec3(1.0, 1.0, 1.0)** - 3-component vector with values x=1, y=1, z=1




## General algorithm

General shading algorithm can be represented using following pseudo-code:

{% highlight python linenos %}
# normal inverse square light fall-off
def attenuation (frag_pos: vec3, light: Light):
  dist = length(frag_pos, light.position);
  return 1 / (dist*dist)

def radiance (light):
  return light.color * attenuation(frag_pos, light)

L0 = vec3(0.0, 0.0, 0.0)
for (light in lights):
  L0 += BRDF(camera_position, light.position) * radiance(light) * dot(N, L)

ambient = vec3(0.03) * albedo * ao; # no pure black

return ambient + L0; # may want to HDR->LDR and gamma this
{% endhighlight %}
*General pixel shader algorithm for PBR*


Attenuation is just a light fall-off that describes how *strong* the incoming light is at a certain distance from light source. Point closer to light source is more affected by light than the one hundreds meters away due too e.g. scattering in the atmosphere. The most well known fall-off function is inverse square of the distance:

$$ distance(light, p) = \vert light\_position - p \vert $$

$$ attenuation(light, p) = \frac {1}{distance^2} $$

Another example is the one used in UE4 ([1], formula 9 ):

$$ attenuation(light, p) = \frac {saturate(1 - (distance / light\_radius)^4)^2} {distance^2 + 1} $$

Or just forgo it altogether:

$$ attenuation(light, p) = 1 $$

Using attenuation we can calculate the radiance for particular point light, that is then multiplied by BRDF and $$dot(N, L)$$. Since $$dot(N, L)$$ is part of practically every shading model, only thing left to talk about is BRDF - we will see the equations in paragraphs that follow.

You may have noticed that we add ambient color to the final result. It's not a physical approach, just a trick to make images look better. It useful to add some light to dark areas (as pure black color does not exist in nature).

It's worth mentioning that this result is not clamped to any particular set of values. We have to manually do the conversion from HDR to LDR. The ever popular reinhard tonemaping operator[2]:

$$ L_d (x,y) = \frac {L(x,y)}{1+L(x,y)} $$

If we want to display the final result we may want to gamma encode it first ([3]):

$$ L_{gamma} (x,y) = {L_d (x,y)} ^ {\frac{1}{2.2}} $$

After that we should be ready to present the image.




## BRDF and material representation

Bidirectional Reflectance Distribution Function (BRDF) is a function that describes how the surface reacts to the incoming light. We used BRDF in the following expression:

{% highlight python linenos %}
for(light in lights):
  L0 += BRDF(camera_position, light) * radiance(light) * dot(N, L)
{% endhighlight %}
*We have to calculate response to each light that exists in the scene*

Both radiance and dot(N, L) does not depend on the properties of the material the surface is made of (though normal vector may be influenced by normal map). All that stuff is hidden inside BRDF. BRDF is a function of 2 directions over hemisphere. To represent direction over hemisphere we have to provide 2 angles per direction, which means that BRDF is a function of total 4 parameters. Alternatively, we can represent directions as vectors $$ f_r (\theta_l, \phi_l, \theta_v, \phi_v) = f_r (l,v)$$. Each material has slightly different BRDF, depending on the material properties.


![BRDF angles]({{image_dir}}/hemisphere.png){: class="image-schematic"}
*BRDF angles. **n** is normal vector of a surface, **t** is tangent vector, **v** is view direction vector, **l** is light direction vector.*


Here is how Disney represents each material ([4]):

* baseColor
* subsurface - used for subsurface scattering (skin, wax etc.)
* metallic
* specular
* specularTint
* roughness
* anisotropy
* sheen - for use with cloth
* sheenTint
* clearcoat - additional specular lobe
* clearcoat Gloss

For comparison, main properties of material in UE4 ([1]):

* baseColor
* metallic
* roughness
* cavity - shadowing from small geometry like seams in clothing
* subsurface - special case shader
* anisotropy - special case shader, seems to not be used very frequently
* sheen - special case shader, seems to not be used very frequently
* clearcoat - special case shader, seems to not be used very frequently

Main advantage of defining materials based on physical properties of their real life counterparts, is that they should look plausible in any lighting conditions. That way, it helps with asset reuse. Artists from The Order: 1886 described in great detail their material pipeline in [5]. Using global shared material library (with materials like cmn_copper_a_tile_dark, cmn_copper_a_tile_worn_light, cmn_copper_a_tile_pristine_light etc.) they mix between material layers using masks (often using generic mask textures shared between many objects).

Unfortunately, it may be somewhat difficult to create non-physically based materials, like noted in instance of Fortnite and Wreck it Ralph [1].




## Framework for physically-plausible BRDF

For BRDF to be physically plausible, it should satisfy following conditions:

* Reciprocity: if we switched light and eye position it should not affect results ($$ f(l,v)=f(v,l) $$)
* Energy conservation: total outgoing light energy should never exceed the incoming light energy (unless surface is emissive)

Most of popular BRDFs assume that resulting color is derived from two components: diffuse and specular. Diffuse is not affected by viewer direction and can be often though as *base* color of an object (at least for dielectrics). Specular depends on the viewer angle and often produces visible highlight. John Hable did an excellent work to show the relation between both of these terms in his [blog post](//filmicworlds.com/blog/everything-is-shiny/). You can read more about the process [here](//filmicworlds.com/blog/how-to-split-specular-and-diffuse-in-real-images/).


![Extracted diffuse and specular components from image]({{image_dir}}/diff-spec--scissors.jpg)
*Image a) presents scissors. Image b) shows the extracted diffuse component. As shown, the handle has orange as base color. Specular component is visible on c), and it does not show any color whatsoever - it displays the specular highlights instead.*


![Diffuse vs specular]({{image_dir}}/diffuse-vs-specular.png){: class="image-schematic"}
*Diffuse component of BRDF does not depend on view angle. Specular component depends on both view angle and surface roughness*


Per [1] we are going to use the following formula:

$$ f_r (l,v) = k_d * F_{Lambert} + k_s * F_{Cook-Torrance} $$

It shows the result of BRDF as being sum of diffuse ($$k_d * F_{Lambert}$$) and specular ($$k_s * F_{Cook-Torrance}$$) terms. $$k_d, k_s$$ describe amount of incoming radiance that is respectively diffused or specularly reflected. For dielectric, both diffuse and specular are important. In case of metals, diffuse is often black and all visible color comes from specular ($$k_d \sim= 0.0, k_s \sim= 1.0$$). Equations to derive $$k_d, k_s$$ will be given after we understand Fresnel term of $$ F_{Cook-Torrance} $$, as they are usually directly correlated. Due to energy conservation following should hold true: $$k_d + k_s \leq 1$$.

Equations for used diffuse and specular models:

$$ F_{Lambert} (l,v) = \frac {albedo}{\pi} $$

$$ F_{Cook-Torrance} = \frac
  {F(l,h) * G(l,v,h) * D(h)}
  {4 * dot(n,l) * dot(n,v)}
$$

Lambertian diffuse is extremely simple, so we won't waste time discussing it. On the other hand, $$ F_{Cook-Torrance} $$ microfacet **specular** shading model is probably a reason why You are reading this article. We will go over each term separately, but first we have to introduce microfacets.




## Microfacets

Most of popular, physically-based shading models assume that large-scale BRDF is governed by small-scale roughness. This the core idea behind the microfacets.


![Microfacets BRDF - vectors]({{image_dir}}/general_vectors.png){: class="image-schematic"}
*Notation for microfacets BRDF: **l** is reversed vector of light direction, **v** is reversed vector of view direction, **h** is halfway vector between v and l, **m** is normal of microfacet, **n** is surface normal. As we will soon discover, the light ray would reflected around microfacet normal m, not surface normal n.*


Each microfacet has it's own normal denoted **m**. Normalized vectors **l**, **v** point toward respectively light source and camera. Since we are looking for a microfacets that would reflect the light ray straight into camera, we can observe that expected m would be equal to $$ h = \frac {l+v} { \vert l + v \vert } $$. Vector h is known as a halfway vector. As the surface consists of uncountable amount of microfacets, some of them will inevitably be oriented just the right way. The question is how many of them?

Microfacets are controlled by material parameter called roughness. Very rough surface will have microfacets in all directions, which means that there always will be at least small group of facets oriented the correct way, no matter how weird the angles get. This results in very larger specular shape. Materials that are glossy (low roughness) usually have much more narrow specular shape.


![Microfacets]({{image_dir}}/other/ndf.png)
*Effect of different roughness on specular highlight. Low roughness means that specular highlight is very sharp, since there is low variance in normals of microfacets. High roughness results in larger highlight.*
{::comment} TODO own image {:/comment}


Microfacet equations heavily depend on angle of both l and v. Having said that, let's go back to $$F_{Cook-Torrance}$$. As we soon shall see, nearly every term of this formula is influenced by microfacet theory.

$$ F_{Cook-Torrance} = \frac
  {F(l,h) * G(l,v,h) * D(h)}
  {4 * dot(n,l) * dot(n,v)}
$$

> Microfacets are much smaller than pixel, too small to even represent using normal maps. That's why some of the equations will sometimes resemble statistical distribution.



## D(h) - Normals Distribution Function

Normal distribution function describes concentration of microfacets that are oriented towards the ideal normal - halfway vector ($$m=h$$). This way incoming light is reflected directly into camera, resulting in stronger specular highlight. Mirror, as a glossy surface, is a perfect example. Due to low roughness, a lot of microfacets are oriented in same direction as macroscopic surface ($$ m \sim= n$$). This mean that reflections are going to be really sharp, to the point that we are easily able to differentiate reflected objects. In case of mirror, normal distribution function for microfacets looks like this:

$$ D_{mirror}(h) =
\begin{cases}
\infty,  & \text{if h=n} \\
0, & \text{else}
\end{cases}
$$

> Value of normal distribution function is a scalar that is NOT constrained to 0-1!

As discussed before, surfaces with high roughness produce much larger specular shape. Value of D(h) is exactly the reason why such phenomenon exist.

![Normals Distribution Function - roughness - surface comparison]({{image_dir}}/mirror-vs-rough.png){: class="image-schematic"}
*There is only a single point on surface of the mirror where h=m. For rough surfaces, there are many such points. This means that the specular shape for rough surfaces will be much larger.*


There are many models for D(h). Here are a selected few (helper variable $$ \alpha = roughness ^ 2 $$):

* **GGX (Trowbridge-Reitz), per [1]:**

  $$ D_{ggx}(h) = \frac {\alpha^2}
							 {\pi * ( dot(n,h)^2 * (\alpha^2 - 1) + 1) ^2}
  $$

* **Blinn-Phong, per [8]:**

	$$ D_{Blinn}(h) = \frac {1} {\pi * \alpha^2}
									* (dot(n,h)) ^(\frac {2}{\alpha^2} -2)
	$$

* **Beckmann, per [8]:**

	$$ D_{Beckmann}(h) = \frac {1} {\pi * \alpha^2 * (dot(n,h))^4}
									* e ^(\frac {(dot(n,h))^2 - 1} {\alpha^2 * (dot(n,h))^2)})
	$$



## G(l,v,h) - Visibility function

> Other names are: Shadow-Masking Term, Self-shadowing Term, Geometry Term

Before light ray reaches the microfacet, it can be blocked by other microfacet. After hitting the microfacet, light ray can also be blocked by other microfacet. Of course, optimal situation would be for ray to travel unobstructed. This fact is represented using visibility function.


![Shadowing, masking]({{image_dir}}/shadowing-masking.png){: class="image-schematic"}
*Shadowing - light ray does not reach the point where m=h, so this ray does not contribute to the specular. Masking - light ray reflects from the microfacet (where m=h), but does not reach the virtual camera.*


> The value of visibility function is a scalar, constrained between 0-1.

One of the main purposes behind visibility function is energy conservation [9]. There are many models for G(l,v,h). Here are a selected few:

* **implicit, per [8]:**

	$$ G_{implicit}(l,v,h) = dot(n,l) * dot(n,v)$$

	It nicely cancels out denominator of $$F_{Cook-Torrance}$$.

* **Cook-Torrance, per [8]:**

$$ G_{Cook-Torrance}(l,v,h) =
		min(1, \\
			\frac{ 2 (dot(n,h)) (dot(n,v)) }{dot(n,h)}, \\
			\frac{ 2 (dot(n,h)) (dot(n,l)) }{dot(n,h)}
		)
$$

* **Smith, per [10]:**

	It's been said that light ray can be blocked between light and microfacet, and also between microfacet and camera. Smith's equation takes it into consideration and breaks G into two components:

	$$ G_{Smith}(l,v,h) = G_1(l)G_1(v) $$

	where $$G_1$$ can be for example (per [1]):

	$$ k = \frac {(roughness+1)^2} {8} $$

	$$ G_1(v) =\frac {dot(n,v)} {dot(n,v) * (1-k) + k} $$

	Be sure to check [8] for more examples.





## Fresnel - F(l,h)

> Fresnel term has it's uses beside just being a term in $$F_{Cook-Torrance}$$ i.e. we are going to make use of it when calculating $$k_d, k_s$$.

Fresnel term describes the amount of light that reflects from a mirror surface given its index of refraction. Rest of light can be e.g. refracted (meaning it will change direction at go 'into' the surface). The amount of reflected light heavily depend not only on l, but also wavelength. We will represent this fact using RGB triplet. E.g. values of (R=1.00, G=0.71, B=0.29) mean that reflected light will have golden color (in fact, these are values for gold at $$l=0dgr$$).


![Fresnel - reflection vs refraction]({{image_dir}}/fresnel.png){: class="image-schematic"}
*Some part of the incoming energy is refracted, some of it is reflected. Fresnel term describes how much of it is reflected.*


Reflectance can be derived from Fresnel equations, which takes in to account e.g. polarization. This makes them very cumbersome to use. Instead, we will write equation for reflectivity at incidence angle (0dgr) and then provide equation with angle as parameter (per [7]).

$$f_0 = (\frac {n_1 - n_2} {n_1 + n_2}) ^2 = (\frac {1-n} {1+n}) ^2 $$

In above equation $$ n_1, n_2, n $$ mean various index of refraction (IOR). If we assume that $$n_1$$ is air, we can simplify to single parameter - index of refraction of surface. Unfortunately, this information is usually not part of material definition. Instead, following approximation is used:

$$f_{0\_aprox} = (1 - metalic) * vec3(0.04) + metalic * albedo$$

For dielectric (when $$metalic \sim= 0$$) we get static value of [R=0.04, G=0.04, B=0.04], which means neither of wavelengths is reflected particularly strong. For metals (when $$metalic \sim= 1$$) we get value of albedo, which is the base color of an object. This may seem counter intuitive - how can we even see specular highlight if it is same color as base object? You may remember that in our BRDF we had following parameters: $$k_d, k_s$$. In case of metals $$k_d \sim= 0.0, k_s \sim= 1.0$$. This mean that metal object can be often treated as black (negligible diffuse component), with all the color (e.g. golden in case of gold, red in case of copper) actually coming from specular component.

Given either of $$f_0$$ or $$f_{0\_aprox}$$, we have reflectance at incident angle 0dgr. Using Schlick's approximation we can extend this knowledge to other angles.

$$F(f_0) = f_0 + (1 - f_0) * (1 - dot(l, v))^5 $$

> Above formula is better suited for dielectrics, but ones for metals are often too complicated to be of any practical use.

If we visualize the example values of the approximation, we see that it is increasing function, reaching value of ~1.0 for 90dgr. Indeed, if we plot e.g. $$f(x) = 0.04 + (1 - 0.04) * (1 - x)^5 $$ it shows value of ~1.0 for $$x=0$$ (cosinus is equal to 0 for 90dgr), and value of 0.04 for $$x \sim= 1$$ (cosinus is equal 1 for 0dgr.)

![Fresnel reflectance]({{image_dir}}/fresnel_reflectance.jpg)
*Fresnel reflectance as a function of angle of incidence for different substances. Copper and aluminum have different reflectance for different wavelengths, which explains multiple curves.*



## Back to kd and ks

Let's take step back and look at our general formulas:

$$ f_r (l,v) = k_d * F_{Lambert} + k_s * F_{Cook-Torrance} $$

$$ F_{Lambert} (l,v) = \frac {albedo}{\pi} $$

$$ F_{Cook-Torrance} = \frac
  {F(l,h) * G(l,v,h) * D(h)}
  {4 * dot(n,l) * dot(n,v)}
$$

Only thing left now is to define $$k_d, k_s$$. $$k_s$$ would describe the amount of light that is reflected of the surface given angles l, v. Turns out we already calculated this value as Fresnel term F. In other words, $$F_{Cook-Torrance}$$ already includes $$k_s$$, so $$k_s = 1$$.

What about $$k_d$$? It would be logical to assume that $$k_d = vec3(1.0) - k_s$$ due too conservation of energy. In fact some demos out there use this particular version of the equation. Common modification is to multiply to material's metallic property value. It's been mentioned a couple of times that metals often have black diffuse, and all of their visible color comes from specular component. We end up with following formulas [11]:

$$k_s = 1$$

$$k_d = (vec3(1.0) - kS) * (1.0 - metallic)$$

You can find more explanation on $$k_d, k_s$$ in section 9.2.2 of [12].



## BRDF in 'Remember Me'

'Remember Me' was one of the earliest titles to make use of physically based rendering. Their whole process is extensively described in series of blog posts found [here](https://seblagarde.wordpress.com/2013/06/07/fxguide-game-environment-series-based-on-remember-me/). Used BRDF for direct light was:

$$ f_{r}(l,v) =
  \frac {albedo} {\pi}
  + F_{schlick}(specular,l,h) \\
    * \frac {SpecPower+2} {8 \pi}
    * (dot(n,h))^{SpecPower}
$$

The specular component is also known as Blinn microfacet specular BRDF. I strongly recommend to read all the articles in the series, especially ones regarding rendering of wet surfaces.



## Extending framework for image-based lighting

> TODO add note for IBL, fix, reformat, add equations incl. the one in 1 section of article

There is a fantastic tutorial on image-based lighting on [learnopengl.com](https://learnopengl.com/PBR/IBL/Specular-IBL). Since the notation matches, it should be easy enough to understand. See [1] for the original idea. I will now proceed to draft a gist of it.

The general algorithm for discrete lights was provided as follows:

$$ sum $$
{::comment} TODO equations {:/comment}

In this equation we iterate over all lights, counting radiance from each one. In real world, the radiance does not come just directly from lights, as light bounces around from all objects. This makes object be *aware* of their environment and makes them *sit* better in their surrounding. This phenomenon is called Global Illumination. One of the tricks used to simulate, GI is by using image-based lighting. It's based on a 3 key ideas:

1. An image can be wrapped completely **around** the scene. Similar effect can be seen on following videos: [Fort Minor - Welcome](https://www.youtube.com/watch?v=REAwGmv0Fuk) (if You stop the video, You will have total control over the camera), [360Â° Video, Manhattan, New York, USA, 4K aerial video](https://www.youtube.com/watch?v=YM6GTu_RcWM). If you are unfamiliar with videos like this, use Your mouse to look around the scene.
2. Every pixel of a image is a light source (or, in other words, every pixel of an image is a measurement of light incoming at that position)
3. The image which provides light is in HDR

{::comment} TODO add sample cubemap {:/comment}

So, instead of sum over all lights, we will sum all radiance over hemisphere around surface's normal (this is called irradiance). We denote this with an integral:


$$ L_o(p,\omega_o) =
  \int\limits_{\Omega}
    	(k_d\frac{c}{\pi} + k_s\frac{DFG}{4(\omega_o \cdot n)(\omega_i \cdot n)})
    	L_i(p,\omega_i) dot(n, w_i)  d\omega_i
$$

Due to the addition we can split the integral into diffuse and specular components:

$$ L_d(p,\omega_o) =
		\int\limits_{\Omega} (k_d\frac{c}{\pi}) L_i(p,\omega_i) dot(n, w_i)  d\omega_i
$$

$$ L_s(p,\omega_o) =
		\int\limits_{\Omega}
      (k_s\frac{DFG}{4(\omega_o \cdot n)(\omega_i \cdot n)})
			L_i(p,\omega_i) dot(n, w_i)  d\omega_i
$$

The problem is, solving this integral in real-time is not possible.  Fortunately, there are some tricks, we can use to fake it.

Turns out, for diffuse component, all we have to do is to precompute part of the equation offline and store the result in separate cubemap. We already have radiance from direction l, all we need to do is for each possible normal vector n calculate irradiance from hemisphere around n. Of course, due to limitations of number of pixels on a cubemap, there are not that many normals to calculate for. During precomputing step we run pixel shader for each pixel of resulting cubemap, with leads to quite intuitive implementation (see **learnopengl.com** for example implementation). The result is then read during run time and provided to following equation:

$$ L_0(p,w_0) =
      k_d * \frac {c} {\pi} *
      \int\limits_{\Omega} L_i(p, \omega_i) * dot(n, \omega_i) d\omega_i
      =
      k_d * \frac {c} {\pi} * texture(precomputedIrradiance, cubemap_coord(n))
$$

All that was possible only because $$ k_d * \frac {c} {\pi} $$ is a constant not depending on angle n,l. This is not a case for specular component. Here are 2 first steps needed:

$$ L_s(p,\omega_o) =
		\int\limits_{\Omega}
      (k_s\frac{DFG}{4(\omega_o \cdot n)(\omega_i \cdot n)})
			L_i(p,\omega_i) dot(n, w_i)  d\omega_i
  \sim=
    \frac {1}{N}
    \sum_{k=0}^{N-1} \frac {L_i(l_k) f(l_k,v) cos \theta_{l_k}} {p(l_k,v)}
  \sim=
    ( \frac {1}{N} \sum_{k=0}^{N-1} L_i(l_k) )
    ( \frac {1}{N} \sum_{k=0}^{N-1} \frac {f(l_k,v) cos \theta_{l_k}} {p(l_k,v)} )
$$

{::comment}
https://agraphicsguy.wordpress.com/2016/09/07/image-based-lighting-in-offline-and-real-time-rendering/
{:/comment}

First step is just using the Monte Carlo to get past the integral. We divide by p (PDF of BRDF) since we would want to use importance sampling to speed up the process. Second step may seem weird ($$\sum a_i b_i \sim= \frac {1}{N} (\sum a_i) (\sum b_i) $$), but apparently is close enough [13]. Or, at the very least, contributes to the error less then the next approximation we are going to take. When discussing difference between [diffuse and specular](# Framework for physically-plausible BRDF), we noted that diffuse component does not depend on viewing angle, while specular does. This makes calculation of $$L_i(l_k)$$ quite problematic. Indeed, [1] suggests just assuming v=n. This means, that if we look at the surface lit by IBL, we would not get the strong reflection we would expect at grazing angles.

![UE4 IBL approximation]({{image_dir}}/ue4 - aprox.jpg)
*Comparison of IBL specular approximation. Original source: [1], see for better resolution*

As we calculate relfection for each normal, we have to determine  (TODO how does ndf play into this?!)

We store result of $$\sum_{k=0}^{N-1} L_i(l_k)$$ in **pre-filtered environment map**. This time we take roughness into considerations: low roughness means sharp, detailed reflections, while high roughness means blurry reflections. As a result, we will create special cubemaps for each roughness (or as many as You would like). It's common to store them in mipmaps, where mipmaps of smaller size contain progressively more blurred values.



{::comment}
list all precomputed maps:
  * Pre-filtered environment map
  * environment BRDF


//diffuse
indirectDiffuse = textureCube(IrradianceMap, refVec)  * diffuseColor

//specular
lod = getMipLevelFromRoughness(roughness)
prefilteredColor =  textureCube(PrefilteredEnvMap, refVec, lod)
envBRDF = texture2D(BRDFIntegrationMap,vec2(roughness, ndotv)).xy
indirectSpecular = prefilteredColor * (specularColor * envBRDF.x + envBRDF.y)

indirectLighting = indirectDiffuse + indirectSpecular
{:/comment}




## Reference

[1] Real Shading in Unreal Engine 4
[2] http://www.cs.utah.edu/~reinhard/cdrom/tonemap.pdf
[3] http://www.codinglabs.net/article_gamma_vs_linear.aspx
[4] Physically-Based Shading at Disney
[5] Crafting a Next-Gen Material Pipeline for The Order: 1886
[6] https://en.wikipedia.org/wiki/Reflection_(physics)
[7] https://en.wikipedia.org/wiki/Fresnel_equations#Normal_incidence
[8] http://graphicrants.blogspot.com/2013/08/specular-brdf-reference.html
[9] s2010_physically_based_shading_hoffman_a_notes
[10] Smith 1967, "Geometrical shadowing of a random rough surface"
[11] https://learnopengl.com/PBR/Theory
[12] Physically based rendering book
[13] https://math.stackexchange.com/questions/337643/split-up-sum-of-products-suma-i-b-i-approx1-n-suma-i-sumb-i-for-uncor

https://seblagarde.files.wordpress.com/2015/07/course_notes_moving_frostbite_to_pbr_v32.pdf

Physically Based Rendering Encyclopedia: https://docs.google.com/document/d/1Fb9_KgCo0noxROKN4iT8ntTbx913e-t4Wc2nMRWPzNk/edit
